{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../') # or just install the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target [1 2 0 0 0 0 0 2 0 0 1 1 1 0 1 1 1 2 2 2 0 0 0 0 1 1 2 0 0 0 0 2 2 1 2 1 1\n",
      " 2 1 0 1 1 0 1 0 1 1 1 2 2 2 0 2 1 1 2 0 1 2 1 0 2 1 2 2 1 0 1 0 1 1 0 1 2\n",
      " 2 2 1 2 2 2 2 1 0 1 1 2 1 2 2 2 1 1 1 0 2 2 1 2 1 0 0 2 0 2 1 2 1 2 1 0 0\n",
      " 2 1 1 2 2 0 2 2 1 2 0 1 0 1 1 2 1 0 1 0 0 1 2 0 1 0 2 0 0 2 0 0 1 1 1 2 1\n",
      " 1 1 1 2 2 0 0 1 1 2 0 1 1 0 0 1 0 1 1 2 0 2 1 2 1 1 1 1 0 1 2 0 2 2 0 0 2\n",
      " 0 2 1 1 2 2 0 2 2 0 1 0 0 2 2] (200,)\n",
      "y_pred_p (200, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = 200\n",
    "class_names = ['a', 'b', 'c']\n",
    "y_target = np.random.randint(0, len(class_names), size=(N))\n",
    "y_pred_p = np.random.uniform(0, 1, size=(N, len(class_names)))\n",
    "y_pred_p = y_pred_p/np.sum(y_pred_p, axis=1)[...,None]\n",
    "print('y_target', y_target, y_target.shape)\n",
    "print('y_pred_p', y_pred_p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "roc_auc_score() got an unexpected keyword argument 'drop_intermediate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5a9fbf529b7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfuzzytools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuteplots\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm_plots\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_custom_confusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmetrics_cdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_multiclass_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#title += f'b-f1score={f1score_xe}'+'\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/oscar/tesis/fuzzy-tools/fuzzytools/datascience/metrics.py\u001b[0m in \u001b[0;36mget_multiclass_metrics\u001b[0;34m(_y_pred_p, _y_target, class_names, metrics, target_is_onehot)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0mbin_bach_cm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBinBatchCM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_target_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf'non-{c}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'c'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_probability_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                         \u001b[0mmetrics_cdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_bach_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'get_{m}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0mmetrics_cdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics_cdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/oscar/tesis/fuzzy-tools/fuzzytools/datascience/metrics.py\u001b[0m in \u001b[0;36mget_aucroc\u001b[0;34m(self, drop_intermediate)\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_bin_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \t\treturn skmetrics.roc_auc_score(self.y_target, p,\n\u001b[0;32m--> 163\u001b[0;31m                         \u001b[0mdrop_intermediate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_intermediate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \t\t\t)\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: roc_auc_score() got an unexpected keyword argument 'drop_intermediate'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from fuzzytools.datascience import metrics as metrics\n",
    "from fuzzytools.cuteplots.cm_plots import plot_custom_confusion_matrix\n",
    "\n",
    "metrics_cdict, metrics_dict, cm = metrics.get_multiclass_metrics(y_pred_p, y_target, class_names)\n",
    "title = ''\n",
    "#title += f'b-f1score={f1score_xe}'+'\\n'\n",
    "#title += f'b-accuracy={accuracy_xe}%'+'\\n'\n",
    "cm_kwargs = {\n",
    "    'title':title[:-1],\n",
    "    'figsize':(6,5),\n",
    "    'normalize_mode':None,\n",
    "}\n",
    "#fig, ax = plot_custom_confusion_matrix(cm[None], class_names, **cm_kwargs)\n",
    "cm_kwargs = {\n",
    "    'title':title[:-1],\n",
    "    'figsize':(6,5),\n",
    "    'normalize_mode':'true',\n",
    "}\n",
    "plot_custom_confusion_matrix(cm[None], class_names, **cm_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_p, y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from fuzzytools.datascience import metrics as metrics\n",
    "from fuzzytools.cuteplots.cm_plots import plot_custom_confusion_matrix\n",
    "\n",
    "metrics_cdict, metrics_dict, cm = metrics.get_multiclass_metrics(y_pred_p, y_target, class_names)\n",
    "print(metrics_cdict[class_names[0]].keys())\n",
    "print(metrics_dict.keys())\n",
    "{c:metrics_cdict[c]['recall'] for c in class_names}\n",
    "for k in metrics_dict.keys():\n",
    "    if 'w-' in k:\n",
    "        continue\n",
    "    print(k, metrics_dict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as skmetrics\n",
    "\n",
    "precision, recall, f1score,_ = skmetrics.precision_recall_fscore_support(y_target, y_pred_p.argmax(axis=-1),\n",
    "                average=None,\n",
    "                labels=range(0, len(class_names)),\n",
    "                )\n",
    "print(np.mean(precision))\n",
    "print(np.mean(recall))\n",
    "print(np.mean(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, score,_ = skmetrics.precision_recall_fscore_support(\n",
    "    np.array([0, 0, 0 ,1 ,0 ,1, 0, 0 ,0, 1, 0, 0, 1, 0, 0, 1 ,0, 0 ,0 ,1]).astype(bool),\n",
    "    np.array([0 ,0, 1, 0 ,0, 1 ,1 ,0, 0, 0, 0, 1, 0, 0, 0, 0, 0 ,0, 0 ,0]).astype(bool),\n",
    "    average='binary', pos_label=1)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rocc = metrics_cdict['a']['rocc']\n",
    "plt.plot(rocc['fpr'], rocc['tpr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rocc = metrics_cdict['a']['prc']\n",
    "plt.plot(rocc['recall'], rocc['precision'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
